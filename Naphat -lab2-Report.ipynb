{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentiment Analysis\n",
    "## 2.2 Movie Review Data\n",
    "\n",
    "Let us first start by looking at the data provided with the exercise. We have positive and negative movie reviews labeled by human readers, all positive and negative reviews are in the ‘pos’ and ‘neg’ folders respectively. If you look in- side a sample file, you will see that these review messages have been ‘tokenized’, where all words are separated from punctuations.\n",
    "There are approximately 1000 files in each category with files names starting with cv000, cv001, cv002 and so on. You will split the dataset into training set and testing set.\n",
    "\n",
    "1. Write some code to load the data from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "Freelist = []\n",
    "list_neg = []\n",
    "list_pos = []\n",
    "File_txt =0\n",
    "Freelist =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "File_txt = glob.glob('review_polarity/txt_sentoken/**/*.txt')\n",
    "\n",
    "for i in File_txt:\n",
    "    REad_File= open(i,'r')\n",
    "    Freelist.append(REad_File.read())\n",
    "    REad_File.close\n",
    "print(len(Freelist))\n",
    "# Freelist\n",
    "# Freelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read  all file in directory ('review_polarity/txt_sentoken/**/*.txt')\n",
    "# we will get neg first and pos second and then we read txt file with open(Filename,'r')\n",
    "# we append word to Freelist and we Get All data neg and pos in Freelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TF-IDF\n",
    "From a raw text review, you want to create a vector, whose elements indicate the number of each word in each document. The frequency of all words within the documents are the ‘features’ of this machine learning problem.\n",
    "\n",
    "A popular method for transforming a text to a vector is called tf-idf, short for term frequencyinverse document frequency.\n",
    "\n",
    "1. Conduct a research about tf-idf and explain how it works.\n",
    "2. Scikit-learn provides a module for calculating this, this is called TfidfVec- torizer.\n",
    "You can study how this function is used here:\n",
    "\n",
    "`http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html`\n",
    "\n",
    "Write code to transform your text to tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x39659 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 666842 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "\n",
    "data_n = TfidfVectorizer()\n",
    "TF_IDF_DATA = data_n.fit_transform(Freelist)\n",
    "TF_IDF_DATA\n",
    "\n",
    "#2.3.1  TF-IDfเป็นการนับจำนวนคำซ้ำๆที่มีอยู่ใน txt ไฟล์แล้วหารด้วยคำทั้งหมดที่มี โดยจะมี TF term คอยเก็บ คำ และ IDF  term ที่คอยเก็บ ค่า weight ของแต่ละคำ โดยคำนวนจากคำที่มีคำซ้ำมากๆ \n",
    "#ก็จะ weight ค่าน้อยเช่น number one,number two ,number three  คำว่า number ก็จะ weight ค่าน้อยเนื่องจาก มีการใช้คำนี้เยอะ\n",
    "#ทำให้การใช้ TF-IDF สามารถลดคำของที่มีความสำคัญน้อย ในข้อมูลของเราลงได้  โดยการคูณ ระหว่าง TF term และ IDF term\n",
    "# fit transform and mean fit() and transform() on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification\n",
    "\n",
    "Use 4 different models to classify each movie into positive or negative category.\n",
    "\n",
    "1. K-Nearestneighbormodel,using module `sklearn.neighbors.KNeighborsClassifier`\n",
    "2. RandomForest, using module `sklearn.ensemble.RandomForestClassifier`\n",
    "3. SVM, using module `sklearn.svm.SVC`\n",
    "4. Neural network, using `sklearn.neural_network.MLPClassifier`\n",
    "\n",
    "You may pick other models you would like to try. Just present results for at least 4 models.\n",
    "Please provide your code for model fitting and cross validation. Calculate your classification accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = ([0]*1000+[1]*1000)\n",
    "# Y_label เป็น label ที่เราจำลองมาจากการอ่านค่า ของไฟล์ข้างบนซึ่งเราเอาค่า ของไฟล์ Neg มาก่อน เลยให้ label ตัวแรกเป็น0 1000ตัว ตาม len ของlist neg\n",
    "#และ เป็น 1 อีก1000 ตัวตาม list ของ pos โดยเราจำลอง LABEL NEG = 0 POS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi =  KNearest\n",
      "ACCURACY  KNearest =  0.572\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.19      0.31      1000\n",
      "          1       0.54      0.95      0.69      1000\n",
      "\n",
      "avg / total       0.67      0.57      0.50      2000\n",
      "\n",
      "Classi =  RandomForest\n",
      "ACCURACY  RandomForest =  0.6565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.77      0.69      1000\n",
      "          1       0.70      0.55      0.61      1000\n",
      "\n",
      "avg / total       0.66      0.66      0.65      2000\n",
      "\n",
      "Classi =  SVM\n",
      "ACCURACY  SVM =  0.7585\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.70      0.74      1000\n",
      "          1       0.73      0.82      0.77      1000\n",
      "\n",
      "avg / total       0.76      0.76      0.76      2000\n",
      "\n",
      "Classi =  NeuralNetwork\n",
      "ACCURACY  NeuralNetwork =  0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.81      0.82      1000\n",
      "          1       0.82      0.84      0.83      1000\n",
      "\n",
      "avg / total       0.82      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLassification = {\"KNearest\":KNeighborsClassifier() ,\"RandomForest\":RandomForestClassifier(), \"SVM\":SVC(), \"NeuralNetwork\":MLPClassifier()}\n",
    "for name,classi in CLassification.items():\n",
    "    Model =cross_val_predict(classi , TF_IDF_DATA, y_label)\n",
    "    print(\"Classi = \",name) \n",
    "    print(\"ACCURACY  \"+ name +\" = \" ,accuracy_score(y_label,Model))\n",
    "\n",
    "    print(classification_report(y_label,Model))\n",
    "    \n",
    "#การใช้ sklearn.model_selection.cross_val_predict(estimator,x,y) คือจะมี3ตัวแปรคือ \n",
    "#estimator เป็น object ทำ fit กับ กับ predict model\n",
    "#x จะเป็นค่า Arrayที่เอาไว้ fit เช่น list หรือ arrayที่มีขั้นต่ำ 2มิติขึ้นไป\n",
    "#y จะเป็นค่าที่จะไว้ พยายาม จะ predict ค่าใหม่ ใน case ของ  supervised learning\n",
    "#การใช้ sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, \n",
    "#sample_weight=None, digits=2)\n",
    "#จะมี 2ตัวแปรหลักๆที่ใช้ คือ y_trueกับ ypred\n",
    "# โดย y_true คือ ค่า correct Value\n",
    "#ส่วน ypred จะเป็นค่าที่ Estimate มาจาก target เพื่อนำมาเทียบกัน และหาค่าต่างๆ อย่างเช่น Precision recall f1-score \n",
    "#ซึ่งค่าต่างๆ  ของแต่ละ คำตอบ อยู่ตามข้างล่างนี้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Model Tuning\n",
    "\n",
    "Can you try to beat the simple model you created above? Here are some things you may try:\n",
    "\n",
    "* When creating TfidfVectorizer object, you may tweak sublinear_tf parameter which use the tf with logarithmic scale instead of the usual tf.\n",
    "* You may also exclude words that are too frequent or too rare, by adjusting max_df and min_df.\n",
    "* Adjusting parameters available in the model, like neural network structure or number of trees in the forest.\n",
    "\n",
    "Design at least 3 experiments using these techniques. Show your experimental results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set_Sublin_Set_Tresh = TfidfVectorizer(sublinear_tf = True ,max_df = 0.6, min_df = 0.03)\n",
    "\n",
    "TF_IDF_DATA_NEW = Set_Sublin_Set_Tresh.fit_transform(Freelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sublinaer_tf = superlinear rates of convergence ของ tf โดย replace จาก tf เป็น tf = 1+log(tf) ทำให้ค่าrate มากขึ้น\n",
    "#max_df = ปรับค่า Threshold จาก 1.0(default) เป็น0.6 ตัดค่าที่มี ค่าซ้ำมากเกิน ไป ออกจาก document \n",
    "#              0.6means \"ignore terms that appear in more than 60% of the documents\".\n",
    "#min_df = ปรับค่า Threshold จาก 1(default) เป็น0.03 ตัดค่าที่มีน้อยเกินไป จน ทำให้ไม่มีผลต่อ document \n",
    "#              0.03 ignore terms that appear in less than 3% of the documents\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi =  KNearest\n",
      "ACCURACY  KNearest 0.7675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.66      0.74      1000\n",
      "          1       0.72      0.88      0.79      1000\n",
      "\n",
      "avg / total       0.78      0.77      0.76      2000\n",
      "\n",
      "Classi =  RF\n",
      "ACCURACY  RF 0.8055\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.85      0.81      1000\n",
      "          1       0.84      0.76      0.80      1000\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2000\n",
      "\n",
      "Classi =  Neural\n",
      "ACCURACY  Neural 0.838\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84      1000\n",
      "          1       0.84      0.84      0.84      1000\n",
      "\n",
      "avg / total       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CLassification = {\"KNearest\":KNeighborsClassifier(n_neighbors=70),\"RF\":RandomForestClassifier(n_estimators = 150,random_state=0),\n",
    "                  \"Neural\":MLPClassifier(solver='lbfgs',hidden_layer_sizes=(20,20), random_state=1)}\n",
    "for name,classi in CLassification.items():\n",
    "    if name ==\"Neural\":\n",
    "        Model =cross_val_predict(classi , TF_IDF_DATA, y_label)\n",
    "        print(\"Classi = \",name) \n",
    "        print(\"ACCURACY  \"+ name ,accuracy_score(y_label,Model))\n",
    "        print(classification_report(y_label,Model))\n",
    "    else:\n",
    "        Model =cross_val_predict(classi , TF_IDF_DATA_NEW, y_label)\n",
    "        print(\"Classi = \",name) \n",
    "        print(\"ACCURACY  \"+ name ,accuracy_score(y_label,Model))\n",
    "        print(classification_report(y_label,Model))\n",
    "    \n",
    "#การใช้ sklearn.model_selection.cross_val_predict(estimator,x,y) คือจะมี3ตัวแปรคือ \n",
    "#estimator เป็น object ทำ fit กับ กับ predict model\n",
    "#x จะเป็นค่า Arrayที่เอาไว้ fit เช่น list หรือ arrayที่มีขั้นต่ำ 2มิติขึ้นไป\n",
    "#y จะเป็นค่าที่จะไว้ พยายาม จะ predict ค่าใหม่ ใน case ของ  supervised learning\n",
    "#การใช้ sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, \n",
    "#sample_weight=None, digits=2)\n",
    "#จะมี 2ตัวแปรหลักๆที่ใช้ คือ y_trueกับ ypred\n",
    "# โดย y_true คือ ค่า correct Value\n",
    "#ส่วน ypred จะเป็นค่าที่ Estimate มาจาก target เพื่อนำมาเทียบกัน และหาค่าต่างๆ อย่างเช่น Precision recall f1-score \n",
    "#ซึ่งค่าต่างๆ  ของแต่ละ คำตอบ อยู่ตามข้างล่างนี้\n",
    "\n",
    "#เหมือนข้อข้างบนแต่มี จุดเปลี่ยนอยู่หลายจุด เช่น\n",
    "# ------KNeighborsClassifier ------\n",
    "#ปรับค่า n_neighbors  จาก 5(default) เป็น70 โดยสนใจตัวรอดข้างมากขึ้น ทำให้ค่า ที่ระหว่าง 0 กับ 1 ในช่วง 70 ตัว จะมีผลต่อค่า Data มากขึ้น และถ้าหาก\n",
    "#เป็น ค่า 0 1 โดยการ random แล้ว n_neighbors จะมีผลมากๆ และบางครั้งอาจจะทำให้ข้อมูล overfit โดยต้องเลือกการใช้ค่านี้ดีๆ แต่ในกรณีนี้ เป็น 0 กับ 1 ที่เป็นค่าเรียงกัน\n",
    "#จึงมีผลกับ 0 1 ในช่วง70 ตัว\n",
    "# ------RandomForestClassifier -----\n",
    "#n_estimators จาก 10(default) เป็น 150 คือจาก สุ่มแค่เพียง10 tree\n",
    "#เปลี่ยนเป็น 150 เพื่อหา อันที่ดีมากยิ่งขึ้น ทำให้เพิ่มประสิทธิภาพการทำงาน\n",
    "#และ set random_state จาก None (default) เป็น 0\n",
    "#เพราะ ตอนแรก หากเป็น None จะrandom ไปเรื่อยๆตาม numpy.random  เราเลย ฟิคค่าไว้\n",
    "#เพื่อเพิ่มประสิทธิภาพของการทำงาน\n",
    "# ------Neural Network ------\n",
    "#ปรับ solver จาก adam(default) เป็น lbfgs \n",
    "# adam จะ ทำงานได้ดีใน Dataที่มันใหญ่ๆมากๆ  (thousands of training sample หรือ มากกว่านั้น) จะดี\n",
    "# lbfgs ทำงานได้ดีใน small dataset faster and perform better ****เร็วกว่ามากๆ\n",
    "# hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
    "# เปลี่ยนจาก 100,length เป็น (20,20) ทั้งหมดเพื่อเพิ่มค่า ความแม่นย่ำ มาจากการทดลองหลายๆแบบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Text Clustering\n",
    "We have heard about Google News clustering. In this exercise, we are going to implement it with Python.\n",
    "\n",
    "## 3.1 Data Preprocessing\n",
    "Let’s switch up and use another dataset called 20newsgroup data, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The data is collected from a university’s mailing list, where students exchange opinions in everything from motorcycles to middle east politics.\n",
    "\n",
    "1. Import data using sklearn.datasets.fetch_20newsgroups \n",
    "2. Transform data to vector with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "SetTrain = fetch_20newsgroups(subset='train')\n",
    "TFidVectorizer = TfidfVectorizer(max_df=0.6, min_df=0.03)\n",
    "Data = TFidVectorizer.fit_transform(SetTrain.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset from sklearn.datasets.fetch_20newsgrups\n",
    "#ใช้ TFidVectorizer จากการอธิบายในข้อ 2.3 และเปลี่ยนมันเป็น vector โดยใช้ .fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Clustering\n",
    "We are going to use the simplest clustering model, k-means clustering, to do this task. Our hope is that this simple algorithm will result in meaningful news categories, without using labels.\n",
    "\n",
    "1. Fit K-Means clustering model to the text vector. What is the value of K you should pick? Why?\n",
    "2. Use Silhouette score to evaluate your clusters. Try to evaluate the model for different values of k to see which k fits best for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#จาก การเลือกค่า K เราควรจะเลือก ที่20 โดยดูจากData setที่ให้มา  มี 20 category ดังนั้น ค่า kเริ่มต้นควรเป็นที่20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20).fit(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014781858389155434"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value_score= silhouette_score(Data,kmeans.labels_)\n",
    "Value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K DATA =  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]\n",
      "MAXDF DATA =  [0.5 0.6 0.7 0.8 0.9]\n",
      "MINDF DATA =  [0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09]\n"
     ]
    }
   ],
   "source": [
    "k_Data = [int(i) for i in range(2,38)]\n",
    "max_df_Data = np.arange(0.5,1.0,0.1)\n",
    "min_df_Data = np.arange(0.01,0.1,0.01)\n",
    "round_loop = 0\n",
    "print (\"K DATA = \",k_Data)\n",
    "print (\"MAXDF DATA = \",max_df_Data)\n",
    "print (\"MINDF DATA = \",min_df_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620\n"
     ]
    }
   ],
   "source": [
    "Maxscore = [0,0,0,0]\n",
    "round_loop =0\n",
    "for Df_max in max_df_Data:\n",
    "    for Df_min in min_df_Data:\n",
    "        for K_value in k_Data:\n",
    "#             TFidVectorizer = TfidfVectorizer(max_df=Df_max, min_df=Df_min)\n",
    "#             kmeans = KMeans(n_clusters=K_value).fit(Data)\n",
    "#             score = silhouette_score(Data,kmeans.labels_)\n",
    "#             print (score)\n",
    "#             if Maxscore[0] < score:\n",
    "#                 Maxscore[0] = score\n",
    "#                 Maxscore[1] =Df_max\n",
    "#                 Maxscore[2] =Df_min\n",
    "#                 Maxscore[3] =K_value\n",
    "#                 print (Maxscore)\n",
    "#             print (Maxscore)\n",
    "            round_loop += 1\n",
    "print (round_loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Data คือค่าใน n_clusters ,MAXDF DATaและ MINDF DATA คือค่าที่อยู่ใน TfidVectorizer ในการตัดคำออกต่างๆ \n",
    "# จากการลูปทั้งหมด 1620 รอบ อาจจะโชว์ค่าจริงๆไม่ได้ เพียงเพราะ มันคงจะมี outputเยอะมากจนคอมค้าง จึงต้องใช้วิธีอื่น\n",
    "#โดยเลือกจากที่คิดว่าน่าจะเป็นไปได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013330683853750608\n",
      " K  =  2\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.010842512096460402\n",
      " K  =  3\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.0027604108517145553\n",
      " K  =  4\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.004262302721368848\n",
      " K  =  5\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.00576620249091172\n",
      " K  =  6\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.0012661166798503527\n",
      " K  =  7\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.008901928086184771\n",
      " K  =  8\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.007311682133817594\n",
      " K  =  9\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.007865702185601564\n",
      " K  =  10\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.004153211055832921\n",
      " K  =  11\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.009553389392312785\n",
      " K  =  12\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.009748716955176776\n",
      " K  =  13\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.01102962682578062\n",
      " K  =  14\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.012441859047931027\n",
      " K  =  15\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.011393510632826587\n",
      " K  =  16\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.01002632756717323\n",
      " K  =  17\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.012662402027510311\n",
      " K  =  18\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013341151453348974, 0.6, 0.3, 2]\n",
      "0.013697266579315666\n",
      "[0.013697266579315666, 0.6, 0.3, 19]\n",
      " K  =  19\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.013697266579315666, 0.6, 0.3, 19]\n",
      "0.01506498841518331\n",
      "[0.01506498841518331, 0.6, 0.3, 20]\n",
      " K  =  20\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.01506498841518331, 0.6, 0.3, 20]\n",
      "0.014018745071716908\n",
      " K  =  21\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.01506498841518331, 0.6, 0.3, 20]\n",
      "0.013048793578776953\n",
      " K  =  22\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.01506498841518331, 0.6, 0.3, 20]\n",
      "0.014825059633922682\n",
      " K  =  23\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.01506498841518331, 0.6, 0.3, 20]\n",
      "0.016621490864018484\n",
      "[0.016621490864018484, 0.6, 0.3, 24]\n",
      " K  =  24\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.016621490864018484, 0.6, 0.3, 24]\n",
      "0.017818939633817466\n",
      "[0.017818939633817466, 0.6, 0.3, 25]\n",
      " K  =  25\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.017818939633817466, 0.6, 0.3, 25]\n",
      "0.016828680565491756\n",
      " K  =  26\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.017818939633817466, 0.6, 0.3, 25]\n",
      "0.01707398758888227\n",
      " K  =  27\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.017818939633817466, 0.6, 0.3, 25]\n",
      "0.0196453786431104\n",
      "[0.0196453786431104, 0.6, 0.3, 28]\n",
      " K  =  28\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.0196453786431104, 0.6, 0.3, 28]\n",
      "0.017855754389593765\n",
      " K  =  29\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.0196453786431104, 0.6, 0.3, 28]\n",
      "0.018772045326412506\n",
      " K  =  30\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.0196453786431104, 0.6, 0.3, 28]\n",
      "0.018850252515179654\n",
      " K  =  31\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.0196453786431104, 0.6, 0.3, 28]\n",
      "0.02103178845252741\n",
      "[0.02103178845252741, 0.6, 0.3, 32]\n",
      " K  =  32\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.02103178845252741, 0.6, 0.3, 32]\n",
      "0.01910364248139966\n",
      " K  =  33\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.02103178845252741, 0.6, 0.3, 32]\n",
      "0.02118464123938383\n",
      "[0.02118464123938383, 0.6, 0.3, 34]\n",
      " K  =  34\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.02118464123938383, 0.6, 0.3, 34]\n",
      "0.022683834554311805\n",
      "[0.022683834554311805, 0.6, 0.3, 35]\n",
      " K  =  35\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "0.020648792749358304\n",
      " K  =  36\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "0.017589984219730338\n",
      " K  =  37\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "round_loop = 0\n",
    "for K_value in k_Data:\n",
    "    TFidVectorizer = TfidfVectorizer(max_df=0.6, min_df=0.03)\n",
    "    Data = TFidVectorizer.fit_transform(SetTrain.data)\n",
    "    kmeans = KMeans(n_clusters=K_value).fit(Data)\n",
    "    score = silhouette_score(Data,kmeans.labels_)\n",
    "    print (score)\n",
    "    if Maxscore[0] < score:\n",
    "        Maxscore[0] = score\n",
    "        Maxscore[1] = 0.6\n",
    "        Maxscore[2] = 0.3\n",
    "        Maxscore[3] = K_value\n",
    "        print (Maxscore)\n",
    "    print (\" K  = \",K_value)\n",
    "    print (\"BEST SCORE  [Value ,Maxdf,Mindf ,K]\" ,Maxscore)\n",
    "    round_loop += 1\n",
    "print (round_loop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_Data_2 = [int(i) for i in range(37,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01999320490146477\n",
      " K  =  37\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "0.02226172687844214\n",
      " K  =  38\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "0.01968501412113009\n",
      " K  =  39\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "0.017715456285546622\n",
      " K  =  40\n",
      "BEST SCORE  [Value ,Maxdf,Mindf ,K] [0.022683834554311805, 0.6, 0.3, 35]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for K_value in k_Data_2:\n",
    "    TFidVectorizer = TfidfVectorizer(max_df=0.6, min_df=0.03)\n",
    "    Data = TFidVectorizer.fit_transform(SetTrain.data)\n",
    "    kmeans = KMeans(n_clusters=K_value).fit(Data)\n",
    "    score = silhouette_score(Data,kmeans.labels_)\n",
    "    print (score)\n",
    "    if Maxscore[0] < score:\n",
    "        Maxscore[0] = score\n",
    "        Maxscore[1] = 0.6\n",
    "        Maxscore[2] = 0.3\n",
    "        Maxscore[3] = K_value\n",
    "        print (Maxscore)\n",
    "    print (\" K  = \",K_value)\n",
    "    print (\"BEST SCORE  [Value ,Maxdf,Mindf ,K]\" ,Maxscore)\n",
    "    round_loop += 1\n",
    "print (round_loop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BEST K in this Data set is  35\n",
      "Best  Silhouette score is  0.02122747632251899\n"
     ]
    }
   ],
   "source": [
    "TFidVectorizer = TfidfVectorizer(max_df=0.6, min_df=0.03)\n",
    "Data = TFidVectorizer.fit_transform(SetTrain.data)\n",
    "kmeans = KMeans(n_clusters=Maxscore[3]).fit(Data)\n",
    "score = silhouette_score(Data,kmeans.labels_)\n",
    "\n",
    "print (\" BEST K in this Data set is \",Maxscore[3])\n",
    "print (\"Best  Silhouette score is \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Topic Terms\n",
    "We want to explore each cluster to understand what news articles are in the cluster, what terms are associated with the cluster. This will require a bit of hacking.\n",
    "1. Use TfidfVectorizer.get feature names to extract words associated with each dimension of the text vector.\n",
    "2. Extract cluster’s centroids using kmeans.cluster centers .\n",
    "3. For each centroid, print the top 15 words that have the highest frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allname_list = TFidVectorizer.get_feature_names()\n",
    "#use get_feature_names to get feature name to extractword we will get array for use only index to find word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03908232, 0.01043577, 0.0393467 , ..., 0.00312986, 0.03230858,\n",
       "        0.01605957],\n",
       "       [0.00449062, 0.00483878, 0.01453639, ..., 0.00490331, 0.03234945,\n",
       "        0.01441724],\n",
       "       [0.00797177, 0.00520058, 0.01690675, ..., 0.00237157, 0.04876936,\n",
       "        0.0215497 ],\n",
       "       ...,\n",
       "       [0.00226075, 0.00426246, 0.01007843, ..., 0.00521206, 0.05057354,\n",
       "        0.02125985],\n",
       "       [0.00379712, 0.00106012, 0.00653402, ..., 0.00218854, 0.05215283,\n",
       "        0.02045172],\n",
       "       [0.00328108, 0.00524984, 0.01946668, ..., 0.00342005, 0.03418815,\n",
       "        0.01828043]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Centroid = kmeans.cluster_centers_\n",
    "Centroid\n",
    "\n",
    "#Extract cluster's centroids with kmean cluseter centers\n",
    "\n",
    "#cluster_centers_ : array, [n_clusters, n_features]\n",
    "#Coordinates of cluster centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid  0\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['sale', 'offer', 'new', 'distribution', 'price', 'or', 'university', 'best', 'email', 'com', 'usa', '10', 'with', 'sell', '00']\n",
      " ----------------------------------\n",
      "Centroid  1\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['university', 'posting', 'host', 'nntp', 'thanks', 'any', 'have', 'de', 'or', 'you', 'with', 'be', 'me', 'mail', 'if']\n",
      " ----------------------------------\n",
      "Centroid  2\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['hp', 'com', 'you', 'have', 'tin', 'with', 'newsreader', 'are', 'version', 'host', 'nntp', 'posting', 'my', 'article', 'or']\n",
      " ----------------------------------\n",
      "Centroid  3\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['steve', 'com', 'you', 'not', 'writes', 'was', 'article', 'are', 'can', 'they', 'if', 'as', 'ca', 'university', 'be']\n",
      " ----------------------------------\n",
      "Centroid  4\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['god', 'jesus', 'not', 'you', 'we', 'are', 'he', 'be', 'as', 'his', 'but', 'have', 'who', 'with', 'do']\n",
      " ----------------------------------\n",
      "Centroid  5\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['car', 'you', 'my', 'com', 'have', 'with', 'was', 'but', 'be', 'or', 'they', 'are', 'if', 'out', 'me']\n",
      " ----------------------------------\n",
      "Centroid  6\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['org', 'you', 'writes', 'article', 'be', 'not', 'com', 'as', 'will', 'if', 'are', 'have', 'with', 'or', 'can']\n",
      " ----------------------------------\n",
      "Centroid  7\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['window', 'windows', 'you', 'problem', 'com', 'with', 'my', 'when', 'be', 'if', 'can', 'or', 'not', 'an', 'have']\n",
      " ----------------------------------\n",
      "Centroid  8\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['mit', 'internet', 'com', 'you', 'host', 'nntp', 'posting', 'with', 'my', 'be', 'are', 'technology', 'an', 'have', 'if']\n",
      " ----------------------------------\n",
      "Centroid  9\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['card', 'video', 'with', 'windows', 'have', 'an', 'can', 'you', 'at', 'does', 'university', 'my', 'anyone', 'or', 'graphics']\n",
      " ----------------------------------\n",
      "Centroid  10\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['netcom', 'com', 'services', 'you', 'be', 'writes', 'david', 'have', 'not', 'article', 'are', 'with', 'line', 'or', 'if']\n",
      " ----------------------------------\n",
      "Centroid  11\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['be', 'you', 'with', 'have', 'are', 'my', 'or', 'as', 'not', 'but', 'if', 'can', 'at', 'one', 'they']\n",
      " ----------------------------------\n",
      "Centroid  12\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['ca', 'canada', 'university', 'you', 'article', 'have', 'writes', 'was', 'my', 'posting', 'can', 'host', 'nntp', 'are', 'with']\n",
      " ----------------------------------\n",
      "Centroid  13\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['ibm', 'com', 'you', 'not', 'pc', 'are', 'or', 'article', 'disclaimer', 'have', 'writes', 'be', 'posting', 'can', 'if']\n",
      " ----------------------------------\n",
      "Centroid  14\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['cs', 'science', 'computer', 'dept', 'article', 'you', 'writes', 'university', 'with', 'be', 'univ', 'are', 'can', 'but', 'have']\n",
      " ----------------------------------\n",
      "Centroid  15\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['uk', 'ac', 'co', 'you', 'with', 'are', 'university', 'be', 'as', 'or', 'can', 'have', 'not', 'if', 'writes']\n",
      " ----------------------------------\n",
      "Centroid  16\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['was', 'were', 'they', 'by', 'as', 'not', 'had', 'their', 'with', 'have', 'who', 'you', 'are', 'no', 'people']\n",
      " ----------------------------------\n",
      "Centroid  17\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['drive', 'hard', 'disk', 'with', 'my', 'have', 'you', 'or', 'can', 'if', 'mac', 'system', 'not', 'computer', 'be']\n",
      " ----------------------------------\n",
      "Centroid  18\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['washington', 'university', 'host', 'nntp', 'posting', 'you', 'article', 'if', 'distribution', 'writes', 'with', 'have', 'can', 'be', 'me']\n",
      " ----------------------------------\n",
      "Centroid  19\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['nasa', 'gov', 'space', 'article', 'writes', 'was', 'you', 'center', 'research', 'not', 'with', 'host', 'com', 'have', 'at']\n",
      " ----------------------------------\n",
      "Centroid  20\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['cc', 'university', 'you', 'posting', 'nntp', 'host', 'have', 'are', 'with', 'au', 'article', 'at', 'not', 'writes', 'or']\n",
      " ----------------------------------\n",
      "Centroid  21\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['state', 'university', 'posting', 'you', 'host', 'nntp', 'article', 'new', 'have', 'are', 'or', 'my', 'be', 'writes', 'at']\n",
      " ----------------------------------\n",
      "Centroid  22\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['gun', 'you', 'are', 'have', 'as', 'be', 'not', 'control', 'they', 'would', 'by', 'if', 'com', 'my', 'with']\n",
      " ----------------------------------\n",
      "Centroid  23\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['space', 'nasa', 'be', 'by', 'as', 'earth', 'not', 'at', 'was', 'with', 'will', 'you', 'would', 'university', 'have']\n",
      " ----------------------------------\n",
      "Centroid  24\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['00', '10', '34', '11', '14', '12', '15', '25', '18', '17', '20', '30', '24', '19', '35']\n",
      " ----------------------------------\n",
      "Centroid  25\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['he', 'his', 'was', 'him', 'be', 'as', 'but', 'not', 'you', 'with', 'have', 'at', 'year', 'has', 'who']\n",
      " ----------------------------------\n",
      "Centroid  26\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['key', 'chip', 'be', 'will', 'they', 'can', 'as', 'public', 'system', 'are', 'with', 'you', 'bit', 'if', 'or']\n",
      " ----------------------------------\n",
      "Centroid  27\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['andrew', 'pittsburgh', 'you', 'host', 'nntp', 'posting', 'com', 'are', 'or', 'reply', 'be', 'what', 'engineering', 'have', 'your']\n",
      " ----------------------------------\n",
      "Centroid  28\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['you', 'your', 'are', 'if', 'not', 'have', 'be', 'do', 'com', 'what', 'as', 'or', 'can', 'with', 'they']\n",
      " ----------------------------------\n",
      "Centroid  29\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['are', 'we', 'not', 'be', 'as', 'you', 'they', 'people', 'have', 'or', 'if', 'with', 'by', 'there', 'but']\n",
      " ----------------------------------\n",
      "Centroid  30\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['she', 'her', 'was', 'my', 'you', 'be', 'not', 'with', 'com', 'has', 'or', 'so', 'as', 'at', 'what']\n",
      " ----------------------------------\n",
      "Centroid  31\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['team', 'game', 'they', 'games', 'was', 'will', 'win', 'but', 'he', 'have', 'you', 'are', 'ca', 'be', 'year']\n",
      " ----------------------------------\n",
      "Centroid  32\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['com', 'inc', 'you', 'article', 'writes', 'posting', 'sun', 'nntp', 'host', 'my', 'have', 'or', 'are', 'not', 'with']\n",
      " ----------------------------------\n",
      "Centroid  33\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['windows', 'dos', 'files', 'file', 'have', 'with', 'you', 'can', 'program', 'use', 'or', 'com', 'my', 'any', 'run']\n",
      " ----------------------------------\n",
      "Centroid  34\n",
      " -----HIHGEST FREQUENCY WORD -----\n",
      "['access', 'net', 'communications', 'com', 'usa', 'nntp', 'posting', 'host', 'be', 'steve', 'unix', 'you', 'or', 'are', 'distribution']\n",
      " ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "ListName =[]\n",
    "for RoundCount,Data in enumerate(order_centroids):\n",
    "    Top = Data.argsort()[-15:][::-1] \n",
    "    print (\"Centroid \",RoundCount)\n",
    "    for index in Top:\n",
    "        ListName.append(name_all[index])\n",
    "    print (\" -----HIHGEST FREQUENCY WORD -----\")\n",
    "    print (ListName)\n",
    "    print (\" ----------------------------------\")\n",
    "    ListName =[]\n",
    "#enumerate is we can for i in data and use another variable to count round\n",
    "#Listname is List  word  15 words Highest frequency\n",
    "#use argsort to sort valiable to change to index example [0.5,0.0,0.7]\n",
    "# argsort in convert to [0.0,0.5,0.7] and then get index --> 0.0 =0  0.5=1 0.7 =2\n",
    "# argsort [0.5, 0.0 ,0.7] --> [1,0,2] \n",
    "# and we use[-15] [::-1] to inverst array and get only 15 last data it mean Highest Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
